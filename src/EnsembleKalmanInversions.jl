module EnsembleKalmanInversions

using Distributions
using ProgressBars
using Random
using Suppressor
using LinearAlgebra
using EnsembleKalmanProcesses.EnsembleKalmanProcessModule
using EnsembleKalmanProcesses.ParameterDistributionStorage

using ..InverseProblems: n_ensemble, observation_map, forward_map

function lognormal_with_mean_std(mean, std)
    k = std^2 / mean^2 + 1
    μ = log(mean / sqrt(k))
    σ = sqrt(log(k))
    return LogNormal(μ, σ)
end

# Model priors are sometimes constrained; EKI deals with unconstrained, Normal priors.
convert_prior(prior::LogNormal) = Normal(prior.μ, prior.σ)
convert_prior(prior::Normal) = prior
convert_prior(constrained_prior) = @error "We don't know how to transform a $(typeof(constrained_prior)) distribution to a Normal distribution."

# Convert parameters to unconstrained for EKI
forward_parameter_transform(::LogNormal, parameter) = log(parameter)
forward_parameter_transform(::Normal, parameter) = parameter

# Convert parameters from unconstrained (EKI) to constrained
inverse_parameter_transform(::LogNormal, parameter) = exp(parameter)
inverse_parameter_transform(::Normal, parameter) = parameter

mutable struct EnsembleKalmanInversion{I, P, E, M, O, F, S, D}
    inverse_problem :: I
    parameter_distribution :: P
    ensemble_kalman_process :: E
    mapped_observations :: M
    noise_covariance :: O
    inverting_forward_map :: F
    iteration :: Int
    iteration_summaries :: S
    dropped_ensemble_members = D
end

construct_noise_covariance(noise_covariance::AbstractMatrix, y) = noise_covariance

function construct_noise_covariance(noise_covariance::Number, y)
    # Independent noise for synthetic observations
    n_obs = length(y)
    return noise_covariance * Matrix(I, n_obs, n_obs)
end

"""
    EnsembleKalmanInversion(inverse_problem; noise_covariance=1e-2)

Return an object that interfaces with EnsembleKalmanProcesses.jl to iteratively
"solve" the inverse problem

```math
y = G(θ) + η
````

for the parameters ``θ``, where ``y`` is a "normalized" vector of observations,
``G(θ)`` is a forward map that predicts the observations, and ``η ∼ N(0, Γy) is zero-mean
random noise with covariance matrix Γy representing uncertainty in the observations.

By "solve", we mean that the iteration finds  ``θ`` that minimizes the distance
between ``y`` and ``G(θ)``.

The "forward map output" `G` can have many interpretations.
The specific statistics that `G` computes have to be selected for each use 
case to provide a concise summary of the complex model solution that contains the values that 
we would most like to match to the corresponding truth values `y`. In the OSBL context, this summary 
could be, for example, a vector of concatenated `u`, `v`, `b`, `e` profiles at all or some time steps
 of the CATKE solution.

Arguments
=========
- `inverse_problem::InverseProblem`: Represents an inverse problem representing the comparison between
                                     synthetic observations generated by Oceananigans and model
                                     predictions also generated by Oceananigans.

- `noise_covariance` (`AbstractMatrix` or `Number`): normalized covariance representing observational
                                                     uncertainty. If `noise_covariance isa Number` then
                                                     it's converted to an identity matrix scaled by
                                                     `noise_covariance`.
"""
function EnsembleKalmanInversion(inverse_problem; noise_covariance=1e-2)
    free_parameters = inverse_problem.free_parameters
    original_priors = free_parameters.priors

    transformed_priors = [Parameterized(convert_prior(prior)) for prior in original_priors]
    no_constraints = [[no_constraint()] for _ in transformed_priors]
    parameter_distribution = ParameterDistribution(transformed_priors, no_constraints, collect(string.(free_parameters.names)))

    # Seed for pseudo-random number generator for reproducibility
    initial_ensemble = construct_initial_ensemble(parameter_distribution, n_ensemble(inverse_problem); rng_seed = Random.seed!(41))

    # Build EKP-friendly observations "y" and the covariance matrix of observational uncertainty "Γy"
    y = dropdims(observation_map(inverse_problem), dims=2) # length(forward_map_output) column vector
    Γy = construct_noise_covariance(noise_covariance, y)

    # The closure G(θ) maps (N_params, ensemble_size) array to (length(forward_map_output), ensemble_size)
    function G(θ) 
        batch_size = size(θ, 2)
        inverted_parameters = [inverse_parameter_transform.(values(original_priors), θ[:, i]) for i in 1:batch_size]
        return forward_map(inverse_problem, inverted_parameters)
    end

    ensemble_kalman_process = EnsembleKalmanProcess(initial_ensemble, y, Γy, Inversion())

    return EnsembleKalmanInversion(inverse_problem, parameter_distribution, ensemble_kalman_process, y, Γy, G, 0, [], Set())
end

struct IterationSummary{P, E}
    parameters :: P
    mean_square_errors :: E
end

function IterationSummary(parameters, forward_map, observations)
    N_observations, N_ensemble = size(forward_map)

    mean_square_errors = [
        mapreduce((x, y) -> (x - y)^2, +, observations, view(forward_map, m, :)) / N_observations
        for m in 1:N_ensemble
    ]

    return IterationSummary(parameters, mean_square_errors)
end

function drop_ensemble_member!(eki, member)
    parameter_ensemble = eki.iteration_summary[end].parameters
    new_parameter_ensemble = vcat(parameter_ensemble[1:member-1], parameter_ensemble[member+1:end])
    new_ensemble_kalman_process = EnsembleKalmanProcess(new_parameter_ensemble, eki.mapped_observations, eki.noise_covariance, Inversion())

    push!(eki.dropped_ensemble_members, memeber)
    eki.ensemble_kalman_process = new_ensemble_kalman_process

    return nothing
end

"""
    iterate!(eki::EnsembleKalmanInversion; iterations=1)

Iterate the ensemble Kalman inversion problem `eki` forward by `iterations`.
"""
function iterate!(eki::EnsembleKalmanInversion; iterations = 1)

    first_iteration = eki.iteration + 1
    final_iteration = eki.iteration + 1 + iterations

    @suppress begin
        for iter = ProgressBar(first_iteration:final_iteration)
            θ = get_u_final(eki.ensemble_kalman_process) # (N_params, ensemble_size) array
            G = eki.inverting_forward_map(θ) # (len(G), ensemble_size)

            # Save the parameter values and mean square error between forward map
            # and observations at the current iteration
            summary = IterationSummary(θ, G, eki.mapped_observations)

            @show summary.mean_square_errors
            
            eki.iteration = iter
            push!(eki.iteration_summaries, summary)

            update_ensemble!(eki.ensemble_kalman_process, G)
        end
    end

    #=
    # All unconstrained
    params = mean(get_u_final(ensemble_kalman_process), dims=2) # ensemble mean

    # losses = [norm([G([mean(get_u(ensemble_kalman_process, i), dims=2)...])...]  - y) for i in 1:n_iterations] # particle loss
    mean_vars = [diag(cov(get_u(ensemble_kalman_process, i), dims=2)) for i in 1:iterations] # ensemble variance for each parameter

    original_priors = eki.inverse_problem.free_parameters.priors
    params = inverse_parameter_transform.(values(original_priors), params)

    # return params, mean_vars, mean_us_constrained
    mean_us_eki = [[mean(get_u(ensemble_kalman_process, i), dims=2)...] for i in 1:iterations]
    mean_us_constrained = [inverse_parameter_transform.(values(original_priors), mean_u) for mean_u in mean_us_eki]
    return [params...], mean_vars, mean_us_constrained
    =#

    return nothing
end

end # module